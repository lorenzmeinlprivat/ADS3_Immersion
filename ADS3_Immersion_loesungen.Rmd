---
title: "Prüfung Vertiefungswissen ADS Immersion"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Prüfung im Vertiefungswissen

# Actuarial Data Science Immersion

## gemäß der Prüfungsordnung 4
## der Deutschen Aktuarvereinigung e.V.

## Zeitraum: 15.04.2021 bis 15.05.2021

__Hinweise:__
- Die Gesamtpunktzahl beträgt 180 Punkte. Die Prüfung ist bestanden, wenn mindestens 90 (50 %) Punkte erreicht werden.
- Bitte prüfen Sie das Ihnen vorliegende Dokument auf Vollständigkeit. Insgesamt sind 31 Aufgaben enthalten.
- Alle Antworten sind zu begründen und Lösungswege müssen nachvollziehbar sein.
- Mit der Einreichung eines Ergebnisnotebooks erklärt die zu prüfende Person, dass das vorliegende Ergebnisbericht inklusive Code- und Beschreibungsanteilen eigenständig erzeugt worden ist. Verstöße gegen diese Grundlagen können vom Prüfungsausschuss sanktioniert werden und zum Ausschluss von der Prüfung führen.
- Die Struktur des Notebooks muss beibehalten werden. Nutzen Sie für Ihre Antworten die freien Zelle(n), die einem Frageblock folgen.

# 0. Einführende Informationen

### Erläuterungen zur Datengrundlage:

Zusätzlich zu diesem Dokument sollten Ihnen zwei Datensätze vorliegen: cs_train_pk1.csv und cs_test_pk1.csv. Wie die Namen bereits andeuten, soll der erste Datensatz zum Training und der zweite Datensatz zum Testen verwendet werden.

Die Datensätze enthalten verschiedene Schadenmeldungen. Dabei beinhalten die verschiedenen Features (z.B. cat1, cat2, ...) Informationen zum jeweiligen Schaden. Die Zielvariable ist loss.

### Inhaltsangabe:

Mit den Aufgaben innerhalb der folgenden Abschnitte sollen die Daten analysiert werden:

1. Allgemeines und Datenimport

    1.1. Allgemeine Fragen
    
    1.2. Trainings-Daten einlesen und einen ersten Überblick erhalten

2. Explorative Datenanalyse

    2.1. Explorative Datenanalyse der numerischen Merkmale

    2.2. Explorative Datenanalyse der kategoriellen Merkmale

    2.3. Daten für die Modellierung vorbereiten
    
3. Modelle

    3.1. Entscheidungsbaumbasierte Modellensembles 
    
            3.1.1. Random-Forest-Verfahren

            3.1.2. Gradient-Boosting-Verfahren
        
    3.2. Regularisierte Lineare Modelle

    3.3. Vorwärtsgerichtetes Neuronales Netz mit Keras/TensorFlow

4. Blending und Scoring

### Anmerkungen zur Lösung:

Hierzu soll ein *Jupyter*-Notebook oder ein *rmarkdown*-Notebook entsprechend erweitert werden. Die zu verwendenden Programmiersprachen sind *Python* oder *R*. Dabei muss sichergestellt werden, dass aktuelle Software-Versionen verwendet werden (Python 3.7 oder höher, R 3.6 oder höher).

Die Aufgabentexte sollen in das Notebook übernommen und dort in der vorgegebenen Reihenfolge beantwortet werden. Ab Frage 4 sind Codes anzugeben, fehlerfrei auszuführen und die Aufgabenerledigung ist folgendermaßen zu strukturieren: 

1.	Markdown: Aufgabentext
2.	Markdown: Kurze Beschreibung dessen, was im Code gemacht wird.
3.	Der entsprechende, #-kommentierte R- oder Python-Code samt Output, ggf. mehrfach.
4.	Markdown: Erläuterung, Interpretation und Kommentierung gemäß Aufgabenstellung, Beantwortung der Fragen.

Zusätzlich soll das Notebook sinnvoll strukturiert werden, z.B. durch Kapitelüberschriften, Abschnitte und ein Inhaltsverzeichnis.
Alle genannten Verfahren sind konkret und sachgerecht zusammen mit der ggf. erforderlichen Datenaufbereitung (z.B. Kodierungen, Skalierung, Transformation) auf den Datensatz anzuwenden. 

Das vorgegeben Gütemaß ist der mittlere absolute Fehler ("mean absolute error", MAE). Die Güte jedes Modells soll auf Basis einer für alle Modelle einheitlichen Validierungsstichprobe ermittelt, angezeigt und kommentiert werden. 

Die Verwendung von Code-Auszügen aus online verfügbaren Notebooks, die für andere Datensätze entwickelt wurden, ist unter Angabe der jeweiligen Quelle zulässig. 

# 1. Allgemeines und Datenimport

## 1.1 Allgemeine Fragen

### A-01: Welche grundsätzliche gute Praxis sollte beim Arbeiten mit Jupyter Notebooks beachtet werden? (Lernziele 3.2.3, 3.2.4) - __[2&nbsp;Punkte]__

Lösungsvorschlag A-01:

* Ergebnisse von rechenintensiven Schritten zwischenspeichern, damit nicht immer der komplette Code durchlaufen werden muss und unnötige Wartezeiten entstehen
* Verschiedene logische Abschnitte sollten in verschiedene Zellen gepackt werden
* Die Analysen sollten mit ausreichenden Erklärungen kommentiert werden
* Benutzung von version control systems wie z.B. git (gilt natürlich nicht nur für Jupyter Notebooks sondern für so gut wie alle Data Science Projekte die man durchführt)


### A-02: Wie kann erreicht werden, dass das eingereichte Notebook reproduzierbar lauffähig ist? (Lernziele 3.2.3, 3.2.4, 3.4.5) - __[3&nbsp;Punkte]__

Lösungsvorschlag A-02:

* Abhängigkeiten angeben und genau spezifizieren (z.B. verwednete Package Versionen und Version der R/Python Installation)
* Angeben was und woher man von außerhalb in das Notebook importiert (z.B. Daten)
* Beides direkt am Anfang des Notebooks spezifizieren
* **Soll ich noch was zu virtual environments schreiben?**

### A-03: Bei den zugrunde liegenden Daten handelt es sich um einen synthetisierten Datensatz aus dem Bereich der Versicherungsbranche. Er wurde in Anlehnung an öffentlich verfügbare Datensätze, zu denen online zahlreiche Notebooks verfügbar sind, erzeugt. Für die Bearbeitung der Aufgaben ist die Sparte und der konkrete Verwendungszweck im Folgenden irrelevant, das letztliche "Ziel" der Aufgaben besteht darin, die enthaltene Zielgröße möglichst präzise zu prognostizieren. Der Datensatz ist somit im Hinblick auf Einhaltung der kartellrechtlichen Grundlagen unverdächtig. Welche Rechtsnormen oder Vorgaben finden bei echten Daten Anwendung, bevor die Entscheidung getroffen werden kann, dass diese für eine Modellierung herangezogen werden können? (Lernziele 1.1.1, 1.1.2) - __[5&nbsp;Punkte]__

Lösungsvorschlag A-03:

Es findet insbesondere die DSGVO Anwendung. 

** Noch weitere? **

** Auf Details der DSGVO eingehen **


# 1.2. Trainingsdaten einlesen und einen ersten Überblick erhalten

### A-04: Bevor die eigentliche Verarbeitung beginnt, sollen die notwendigen Programmbibliotheken aufgerufen/importiert sowie der Datensatz aus der Datei `cs_train.csv` eingelesen und die ersten Datensätze (vollständig) angezeigt werden. (Lernziel 3.4.5) - __[3&nbsp;Punkte]__

Lösungsvorschlag A-04:

Zuerst werden globale Einstellungen für das Markdown vorgenommen.

```{r global settings}
# Keine Warnings z.B. bei laden der Pakete im Output anzeigen
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 


```


```{r A-04}


# lade die benötigten Pakete
library(caret) # TODO: packages ergänzen
library(dplyr)
library(moments)
library(purrr)
library(PerformanceAnalytics)
library(ggplot2)
library(gridExtra)
library(ranger)
library(lightgbm)
library(catboost)
library(glmnet)
library(MLmetrics)

# lese die Trainingsdaten ein
train_data <- read.csv("./Daten/cs_train_pk1.csv", header = TRUE, sep = "," , 
                       dec = ".", stringsAsFactors = TRUE)

# Darstellung der ersten Datensätze
head(train_data)

```

Der Datensatz verfügt über sehr viele (75) kategorielle Merkmale, die über Buchstaben codiert sind. Zudem bestehen zehn stetige Variablen, die anhand der ersten Werte bereits den Eindruck machen, dass sie normiert sind. Die Werte der Zielvariable loss, die angezeigt werden sind im mittleren 1000er Bereich.  

### A-05: Wie viele Schadenfälle und Merkmale hat der Datensatz? (Lernziele 3.3.1, 3.4.1) - __[1&nbsp;Punkt]__

Lösungsvorschlag A-05:

Über die Dimension des Datensatzes kann die Anzahl der Schadenfälle und Merkmale ermittelt werden.

```{r A-05}
anzahl_schadenfaelle <- dim(train_data)[1]
anzahl_schadenfaelle
anzahl_merkmale <- dim(train_data)[2]
anzahl_merkmale

```
Der Datensatz enthält `r anzahl_schadenfaelle` Schadenfälle und `r anzahl_merkmale` Merkmale.

### A-06: Das Merkmal 'id' soll entfernt werden. Darüber hinaus ist zu prüfen, ob und wenn ja, wie viele Werte fehlen. Welche Schlussfolgerung ergibt sich daraus? (Lernziele 3.3.2, 3.4.4) - __[1&nbsp;Punkt]__

Lösungsvorschlag A-06:

```{r A-06}
train_data <- train_data %>% dplyr::select(-"id")

# Test, ob es fehlende Werte im Datensatz gibt  
any(is.na(train_data))

```
Es gibt keine fehlenden Werte im Trainingsdatensatz. Für den Trainingsdatensatz muss daher nicht entschieden werden wie mit fehlenden Werten umgegangen wird.

# 2. Explorative Datenanalyse

## 2.1. Explorative Datenanalyse der numerischen Merkmale

### A-07: Für numerischen Merkmalen des Datensatzes sind die Wertebereiche festzustellen. Existiert eine Verteilungsschiefe und wie sieht diese, falls vorhanden, aus? (Lernziele 3.3.2, 3.4.4) - __[5&nbsp;Punkte]__

Lösungsvorschlag A-07:

Zuerst werden zwei weitere Datensätze erstellt. Ein Datensatz mit numerischen und ein Datensatz mit kategoriellen Variablen. Der Datensatz mit den numerischen Variablen wird der Auggabenstellung entsprechend analysiert.

```{r A-07}
# Aufteilung des Datensatzes in numerische und kategorielle Merkmale
train_data_num <- train_data %>% dplyr::select_if(is.numeric)
train_data_cat <- train_data %>% dplyr::select_if(is.factor)

# die Wertebereiche sind aus dem Output der summary() Funktion zu lesen
summary(train_data_num)

# Berechnung der Verteilungsschiefe
purrr::map_dbl(train_data_num, moments::skewness)

```
Die Merkmale `cont1` bis `cont10` weisen teils positive und teils negative Werte auf, wobei die positiven Werte überwiegen und höher ausfallen. Positive Werte bedeuten eine linksschiefe Verteilung, negative Werte eine rechtsschiefe Verteilung. 

Die Zielvariable `loss` weist eine starke Linksschiefe aus. Bei der Zielvariablen gibt es zudem einige sehr hohe Werte. Das Maximum ist beispielsweise um ein Vielfaches höher als Mittelwert und 75% Quantil.

### A-08: Die Zielgröße soll geeignet logarithmisch transformiert werden. Wieso ist das nötig, bzw. sinnvoll? Welche Folgen kann diese Vorgehensweise auf die prognostizierten Werte haben? (Lernziele 3.3.2, 3.4.4) - __[5&nbsp;Punkte]__

Lösungsvorschlag A-08:

Da die Schiefe ungefähr dem Wert von *e* entspricht entscheide ich mich für den natürlichen Logarithmus. 

```{r A-08}
# Transformation mit dem natürlichen Logarithmus 
# (auch im ursprünglichen Datensatz "train_data")
train_data_num$loss <- log(train_data_num$loss)
train_data$loss <- log(train_data$loss)


# Berechnung der Schiefe nach der Transformation
skewness(train_data_num$loss)

# Boxplot der transformierten Zielvariable
boxplot(train_data_num$loss)

```
Nach der logarithmischen Transformationen weist die Zielvariable nur noch eine geringe Schiefe auf und auch der Boxplot sieht gleichmäßig aus.

TODO: recherchieren, warum man transformiert.

Das Trainieren eines Modells auf den logarithmierten Werten liefert bei Anwendung des Modells zur Generierung von Vorhersagen natürlich auch logarithmierten Werte zurück. Die Prognosen müssen also noch entsprechen mit *e* skaliert werden, um die eigentlich gewünschten Vorhersagen zu erlangen. 

### A-09: Es soll im Folgenden die Verteilungsdichte der numerischen Merkmale visualisiert werden. Welche konzeptionellen Möglichkeiten dazu gibt es und wie unterscheiden sich diese? (Lernziele 3.3.2, 3.4.4) - __[5&nbsp;Punkte]__

Lösungsvorschlag A-09:

Die Visualisierung erfolgt unter Verwendung des ggplot Pakets über Histogramme, beziehungsweise die aus den Werten abgeleiteten Verteilungsdichtewerten. Um einen for-loop zu vermeiden definiere ich eine Funktion, die ich dann vektorisiert auf alle Spalten anwende. 

```{r A-09}
# definiere die Funktion, die die Plots erstellt
plot_num_spalte <- function(data, spalte){
  ggplot(data, aes_string(x = spalte)) + 
    geom_histogram(aes(y=..density..), bins = 50, colour = "blue", fill = "white") + 
    geom_density(alpha = 0.2, fill = "deepskyblue4")
}

# Anwenden der Funktion
plots_num <- lapply(colnames(train_data_num), plot_num_spalte, data = train_data_num)

#Anzeigen der Plots
gridExtra::marrangeGrob(plots_num, nrow = 2, ncol = 2)

```

TODO: Interpretation der Plots

Die Verteilungen wurden nur einzeln visualisiert. Weitere Möglichkeiten wäre die gemeinsame Verteilung zweier Variablen über Heat-Maps oder Scatterplots. 
TODO: Weitere Möglichkeiten zur Visualisierung von Verteilungen 


### A-10: Im Weiteren soll die Korrelation zwischen den numerischen Merkmalen einschließlich 'loss' bestimmt werden. Dies ist auch in einer geeigneten Graphik darzustellen. Wieso ist die grafische Darstellung sinnvoll? Was sind die auffälligsten Zusammenhänge? (Lernziel 5.2.3) - __[5&nbsp;Punkte]__

Lösungsvorschlag A-10:

```{r A-10}
# Erzeugen des Korrelationsplots mit Hilfe des PerformanceAnaltics Packages
# corr_plot <- PerformanceAnalytics::chart.Correlation(train_data_num, histogram = TRUE, 
#                                                      method = "pearson", cex = 0.01)


```
Aus der obigen Graphik sind die Pearson Korrelationen abzulesen. 
TODO: Interpretation des plots

## 2.2. Explorative Datenanalyse der kategoriellen Merkmale

### A-11: Die Häufigkeitsverteilungen der kategoriellen Merkmale sind als Säulengraphiken darzustellen. Welche Schlussfolgerungen ergeben sich? (Lernziele 5.2.1, 6.1.2) - __[5&nbsp;Punkte]__

Lösungsvorschlag A-11:

Ich erstelle zuerst wieder eine Funktion, mit dessen Hilfe ich für alle kategoriellen Merkmale das Säulendiagramm erstelle. Die in einer Liste gespeicherten Diagramme plotte ich anschließend.

```{r A-11}
# definiere die Funktion, die die Plots erstellt
plot_cat_spalte <- function(data, spalte){
  ggplot(data, aes_string(x = spalte)) + 
    geom_bar() 
}

# Anwenden der Funktion
plots_cat <- lapply(colnames(train_data_cat), plot_cat_spalte, data = train_data_cat)

# Darstellen der Plots
gridExtra::marrangeGrob(plots_cat, nrow = 4, ncol = 3)

# alternative Darstellung
# cowplot::plot_grid(plotlist = plots_cat, ncol = 5)

```

61 kategorielle Variablen haben nur vier oder weniger Merkmalsausprägungen/Level. Allerdings sind bei vielen der Variablen die Merkmalsausprägungen sehr unterschiedlich verteilt. Beispielsweise existieren bei der Variable `cat4` nur sehr wenige Merkmale der Kategorie `B`. Einige Variablen verfügen über sehr viele verschiedene Level. Die verschiedenen Level können bei der Anwendung von One-Hot-encoding zu einer sehr hohen Anzahl an Dimensionen führen. Die Hohe Anzahl an Variablen erhöht die Rechenzeiten der Algorithmen und birgt die Gefahr schnell in Overfitting zu kommen.

### A-12: Welche Erkenntnisse können aus dem logarithmierten Schadenaufwand gezogen werden? Zudem sollen Boxplot-Diagramme für jedes Attribut der kategoriellen Merkmale dargestellt werden. Was ist daraus abzuleiten? (Lernziele 5.2.1, 6.1.2) - __[5&nbsp;Punkte]__

Lösungsvorschlag A-12:

Ich erstelle zuerst wieder eine Funktion, mit dessen Hilfe ich für alle kategoriellen Merkmale einen Box mit dem logarithmierten Schadenaufwand als y-Wert erstelle. Die in einer Liste gespeicherten Diagramme plotte ich anschließend.

```{r A-12}
# definiere die Funktion, die die Plots erstellt
plot_box_spalte <- function(data, spalte){
  ggplot(data, aes_string(x = spalte, y = "loss")) + 
    geom_boxplot() 
}

# Anwenden der Funktion auf die kategoriellen Merkmale
plots_box <- lapply(colnames(train_data_cat), plot_box_spalte, data = train_data)


# Darstellen der Plots
gridExtra::marrangeGrob(plots_box, nrow = 3, ncol = 3)
```
Die logarithmische Transformation des Schadenaufwands ändert nur die Höhe aber nicht die Ordnung/Reihenfolge der Werte. Der relative Einfluss (also die Richtung des Einflusses) der Merkmalsausprägungen von Variablen auf die Zielvariable kann also immer noch, wenn auch anders skaliert, beobachtet werden. 

Für einige Variablen, wie zum Beispiel `cat7` sind deutliche Unterschiede in der Höhe des logarithmierten Schadenaufwandes in Abhängigkeit der Merkmalsausprägung zu sehen. Dies ist in ähnlicher (wenn auch teils schwächerer) Form für viele weitere Variablen der Fall. Für einige andere Variablen wie `cat49` ist aus dem Boxplot kein Zusammenhang zwischen Merkmalsausprägung und Zielvariable zu sehen. (Hinweis: dies bedeutet aber nicht automatisch, dass die Variable nicht zu einer verbesserten Vorhersage beitragen kann.)

## 2.3. Daten für die Modellierung vorbereiten

### A-13: Der Zufallsgenerator soll sinnvoll und zielführend eingestellt werden. Welche Möglichkeiten zur Kontrolle des "Random State" gibt es dazu und wie wirken sie sich auf die Reproduzierbarkeit aus? (Lernziel 3.3.1, 3.4.1) - __[2&nbsp;Punkte]__

Lösungsvorschlag A-13:

** Möglickeiten zur Kontrolle des RAndom State ** 

```{r A-13}
seed = 12334567

```

### Wenden Sie diesen im weiteren Verlauf zur Erzeugung reproduzierbare Ergebnisse an. Setzen Sie nach Vollendung Ihrer Analyse einen neuen Startwert und überprüfen Sie die Stabilität Ihrer Ergebnisse. 

### A-14: Zur sinnvollen Verwendung der kategoriellen Merkmale in Regressionsmodellen müssen diese (häufig) enkodiert werden. Es soll ein "One-Hot-Encoding" auf die kategoriellen Merkmale angewendet werden. Welche anderen Möglichkeiten des Encodings gibt es und wie unterscheiden sie sich bezüglich Praktikabilität und Performance? Welche möglichen Unterschiede zwischen den Trainings- und Testdaten sind hierbei zu beachten? (Lernziel 3.3.1, 3.4.1) - __[6&nbsp;Punkte]__

Lösungsvorschlag A-14:

```{r A-14}
# ich benutze das caret package
# fullrank = True sorgt dafür, dass bei eine Variable mit 2 Kategorieien nur eine Variable zum encoden
# verwendet wird und somit keine lineare Abhängigkeiten generiert werden
dummy <- caret::dummyVars(" ~ . ", data = train_data_cat, fullRank = TRUE)
train_data_cat_encoded <- data.frame(predict(dummy, newdata = train_data_cat))

```

Weitere Möglichkeiten des Encodings sind:

* Label Encoding: Die Kategorien der Vairable werden durch Nummern von 1 bis n ersetzt, wobei n die Anzahl der verschiedenen Kategorien der Variable sind. Ein Nachteil hierbei ist, dass Label Encoding meistens nicht passend für lineare Modelle ist.

* Frequency Encoding: Die Kategorien werden mit dem Anteil der Beobachtungen der jeweiligen Kategorie an den Gesamtbeobachtungen ersetzt. Wenn eine Kategorie also in 15 % der Beobachtungen vorkommt wird die Kategorie durch den Wert 0,15 ersetzt. Ein Nachteil hierbei ist, dass Kategorien mit der gleichen Anzahl an Beobachtungen mit dem gleichen Wert encoded werden und so womöglich ihre nützliche Information verlieren.  

* Target Encoding (auch Mean Encoding genannt): Hierbei wird der kategoriellen Variable ein numerischer Wert in Abhängigkeit des Einflusses auf die Zielvariable zugewiesen. Dies sorgt teilweise zu besseren Modellqualitäten, allerdings ist die Gefahr groß bei nicht richtiger Anwendung zu overfitten.


** Unterschiede zwischen Trainings- und Testdaten **


### A-15: Das Ergebnis des "One-Hot-Encoding" ist zu erläutern. Was sind die zu erwartenden Folgen? Treten hier Probleme hinsichtlich des "Curse of Dimensionality" auf? Welcher Art sind diese und welcher Umgang sollte damit getroffen werden? (Lernziel 5.1.1) - __[5&nbsp;Punkte]__

Lösungsvorschlag A-15:

Die verschiedenen n Katgegorien einer Variable werden auf (n-1) Spalten aufgeteilt und die Ausprägung mit 0-1 encoded. Dieses Vorgehen wurde für alle kategoriellen Variablen durchgeführt. Dadurch erhöht sich die Dimension des Datensatz von 75 auf 296 Spalten. "Curse of Dimensionality" beschreibt, dass eine steigende Anzahl an Variablen senkt die "Datendichte" und es wird schwieriger für Modelle die tatsächlichen Zusammenhänge in den Daten von Zufällen zu unterscheiden.

Als Umgang bezüglich des "Curse of Dimensionality" könnte eine Dimensionsreduktion, zum Beispiel über eine Hauptkomponentenanalyse durchgeführt werden. Diese reduziert die Anzahl der Dimensionen unter Beibehaltung der meisten Informationen des Datensatzes.

### A-16: Auf Basis der Trainingsdaten sind die üblichen ML-Matrizen X und y für das Modelltraining und für die Modellvalidierung zu erzeugen. (Lernziele 3.3.3, 3.4.5) - __[2&nbsp;Punkte]__

Lösungsvorschlag A-16:

```{r A-16}
# Zielvariable y "loss"
y <- train_data_num$loss

# Feature Matrix ohne loss
X_matrix <- cbind(train_data_cat_encoded, 
                  dplyr::select(train_data_num, - loss))

# einige algorithmen nehmen X und y zusammen in eine matrix
Xy_matrix <- cbind(train_data_cat_encoded, 
                  train_data_num)

```

# 3. Modellierung


### A-17: Es sollen Datenstrukturen vorbereitet werden, die einen komfortablen Vergleich der Modelle und Ergebnisse erleichtern. Worauf gilt es hierbei zu achten und welche Aspekte sind in Bezug auf die Gütemaße bereits vor der Modellierung zu überlegen? Welche Vor- und Nachteile hat das vorgegebene Gütemaß mittlerer absoluter Fehler (MAE) im Vergleich zum mittleren quadratischen Fehler (MSE) im Hinblick auf die Vorhersagegenauigkeit auf individueller Ebene sowie die Erwartungstreue des Schätzers? (Lernziel 4.1.7) - __[5&nbsp;Punkte]__

Lösungsvorschlag A-17:

Zur Bewertung der Algorithmen teile ich den Datensatz in Trainings- und Validierungsdatensatz auf.

Ich definiere eine Funktion, die mir für Vektoren und Vorhersagen verschiedene Metriken zurückgibt, die ich zur Evaluation der Algorithmen verwende. Ein Kriterium davon ist MAE, da unterschiedliche Modelle bei unterschiedlichen Kriterien unterschiedlich abschneiden können, verwende ich zusätzlich noch RMSE, Gini und R^2.

** Aufteilen, damit man einen extra VAlidierungsdatensatz hat? **

```{r A-17}
# Aufteilung in Trainings- und Validierungsdatensatz
# TODO

# definiere die Funktion
evaluation = function(y_pred, y_true){
  MAE = MLmetrics::MAE(y_pred = y_pred, y_true = y_true)
  gini_coefficient = MLmetrics::NormalizedGini(y_pred = y_pred, y_true = y_true)
  RMSE = MLmetrics::RMSE(y_pred = y_pred, y_true = y_true)
  R2 = MLmetrics::R2_Score(y_pred, y_true)
  eval_metric = matrix(c(MAE, RMSE, gini_coefficient, R2), nrow=1, ncol=4, 
                       dimnames = list("Value", c("MAE", "RMSE", "gini", "R2")))
  return(eval_metric)
}

```

** Beantwortung der Fragen **

## 3.1. Entscheidungsbaumbasierte Modellensembles

### 3.1.1. Random-Forest-Verfahren

### A-18: Zunächst soll das Random-Forest-Verfahren verwendet werden. Es sollen die Hyperparameter Baumtiefe und Baumanzahl über eine Gittersuche bestimmt werden. Was sind Hyperparameter, was ist ihre Bedeutung? (Lernziele 4.1.3, 4.1.4) - __[10&nbsp;Punkte]__

Lösungsvorschlag A-18:

Ich verwende das Package `ranger` Package. `ranger` hat den Vorteil, dass es schneller ist als die meisten anderen Random Forest Packages. 

Ich erstelle einen Grid mit verschiedenen Parameterkombinationen und trainiere für jede Kombination ein Modell. Die Modelle bewerte ich anhand des Fehlers der Out-of-bag predictions mit dem MAE Kriterium.

```{r A-18}
# definiere grid
my_grid <- expand.grid(
  num.trees = c(70, 100, 200),
  max.depth = c(1, 3, 5, 7),
  mae = NA
)

# Anzahl der Variablen
n_features <- dim(X_matrix)[2]

set.seed(seed)
ptm <- proc.time() # TODO: zeit anders stoppen
# loop durch die grid parameter Optionen
# erzeuge ein Modell je Kombinationsmöglichkeit
for (i in seq_len(nrow(my_grid))){
  
  model_rf <- ranger::ranger(
  loss ~ .,
  data = Xy_matrix,
  
  # loop durch die grid parameter Optionen
  num.trees = my_grid$num.trees[i],
  max.depth = my_grid$max.depth[i],
  
  # empfohlene Parametereinstellung für Regression
  mtry = floor(n_features / 3),
  
  # Berechnung der Variable Importance
  importance = 'impurity'
  
  )
  
  # speichern der out-of-bag prediction fehler
  my_grid$mae[i] <- model_rf$prediction.error
  
}

time_rf <- proc.time() - ptm

# sortieren der Modelle nach MAE und anzeigen der Werte
sorted_grid <- my_grid %>% arrange(mae) 
sorted_grid 

# Ich trainiere erneute das Beste Modell und Speichere es
model_rf <- ranger::ranger(
  loss ~ .,
  data = Xy_matrix,
  num.trees = sorted_grid$num.trees[1],
  max.depth = sorted_grid$max.depth[1],
  mtry = floor(n_features / 3),
  importance = 'impurity'
  )


```

** Hyperparamter und Ihre Bedeutung **

### A-19: Die Variablen-Importance (Z. 4.1.4) soll berechnet werden und Auskunft über die Eigenschaften des trainierten Lerners geben. Welche Möglichkeiten zur Berechnung und Visualisierung am Beispiel des Random-Forest sind hier zielführend? (Lernziel 4.1.4) - __[8&nbsp;Punkte]__

Lösungsvorschlag A-19:

** Noch auf Fragen eingehen **

```{r A-19}
# extrahieren der Variable importance aus dem Model und Sortierung der Werte 
var_imp_sorted <- as.data.frame(sort(model_rf$variable.importance, decreasing = TRUE))
var_imp_sorted <- tibble::rownames_to_column(var_imp_sorted, "Variable")
var_imp_sorted <- dplyr::rename(var_imp_sorted, 
                                "Importance" = "sort(model_rf$variable.importance, decreasing = TRUE)" )

# Anzeigen der 20 wichtigsten Merkmale
head(var_imp_sorted, 20)

# Visualisierung über einen gedrehten Bar Chart
varImpPlot <- ggplot(head(var_imp_sorted, 20), 
                     aes(x = Importance, y = reorder(Variable, Importance))) +
  geom_bar(stat = "identity", position = "dodge") +
  xlab("Variable Importance")
varImpPlot

```

### 3.1.2 Gradient-Boosting-Verfahren

### A-20: Die hinter den Gradient-Boosting-Tools lightGBM, XGBoost und CatBoost stehende Methodik ist zu beschreiben. Welche Unterschiede, Vor- und Nachteile lassen sich identifizieren? Welches Konzept eignet sich im vorliegenden Fall? (Lernziele 3.2.1, 4.1.7) - __[8&nbsp;Punkte]__

Lösungsvorschlag A-20:

lightGBM zeichnet sich durch schnellere Trainingszeiten und weniger Speicherverbrauch aus. Es daher schneller als die meisten anderen Boosting Algorithmen und auch für große Datensätze geeignet. 

XGBoost hat die größte Community. XGBoost hat keine inbuilt Methode für den Umgang mit kategoriellen Variablen. Das encoding muss daher vom Anwender selbst vorab vorgenommen werden.

CatBoost ist mit Lizenz und hat eine automatische Unterstützung kategorieller Variablen. 

Da wir einen Datensatz mit sehr vielen verschiedenen Merkmalen haben eignet sich lightGBM, da dieser Algorithmus typischerweise der schnellste ist.

```{r}

```

** Warum ist ein Code Block hier? **

### A-21: Es ist eine Hyperparameteroptimierung über eine mehrdimensionale "Randomized Search" für lightGBM durchzuführen. (Lernziele 3.2.1, 4.1.7) - __[8&nbsp;Punkte]__

Lösungsvorschlag A-21:

```{r A-21}

```


### A-22: Es ist eine Hyperparameteroptimierung über eine mehrdimensionale "Randomized Search" für XGBoost durchzuführen. (Lernziele 3.2.1, 4.1.7) - __[8&nbsp;Punkte]__

```{r}

```


### A-23: Es soll Catboost mit Default-Werten angewendet werden. (Lernziele 3.2.1, 4.1.7) - __[4&nbsp;Punkte]__

Lösungsvorschlag A-23:

```{r A-23, message=FALSE}
# Data Set vorbereiten
train_pool <- catboost.load_pool(data = X_matrix, label = y)

# trainieren des Modells
dummy <- capture.output({
  model_catboost <- catboost.train(learn_pool = train_pool)
})
```

### A-24: Es sind die Ergebnisse und Laufzeiten von A-21 bis A-23 zu vergleichen und zu bewerten. (Lernziele 3.2.1, 4.1.7) - __[4&nbsp;Punkte]__

Lösungsvorschlag A-24:

```{r A-24}

```

### A-25: Es soll für die in A-21, A-22 und A-23 trainierten Modelle die "Variable-Importance" berechnet und für die 20 wichtigesten Merkmale visualisiert werden. Welche Unterschiede im Vergleich zum RF-Verfahren ergeben sich und worauf sind diese zurückzuführen? (Lernziel 4.1.4) - __[4&nbsp;Punkte]__

```{r}

```

## 3.2. Regularisierte Lineare Modelle

### 3.2.1 Datenaufbereitung

** Soll ich die Daten normieren? **

```{r Normierung Datensatz}

```

### 3.2.2 Das Lasso-Verfahren

### A-26: Es sind die Gemeinsamkeiten und der Unterschied zwischen den Verfahren Ridge-Regression und Lasso-Regression zu erläutern. Das Lasso-Verfahren soll auf den Daten für verschiedene Komplexitätsvorgaben (via Penalty-Parameter) sachgerecht angewendet und die Ergebnisse interpretiert werden. Es soll gezeigt werden, wie sich in Abhängigkeit vom Penalty-Parameter der Messfehler an der Validierungsstichprobe entwickelt und wie sich dabei die Anzahl der vom Modell nicht berücksichtigten Merkmale (also mit Koeffizient=0) in Abhängigkeit vom Penalty-Parameter verändert. (Lernziel 4.1.2) - __[10&nbsp;Punkte]__

Lösungsvorschlag A-26:

```{r A-26}
# ich trainiere das Modell
# alpha=1 gibt einem die Lasso-Regression 
# 10 fache Kreuzvalidierung zur Bestimmung des Vorhersagefehlers
set.seed(seed)
model_lasso <- cv.glmnet(
  x = as.matrix(X_matrix),
  y = y,
  family = "gaussian",
  alpha = 1,
  standardize = TRUE,
  nfolds = 10
)

# plot der mittleren quadratischer Abweichung in Abhängigkeit von lambda
plot(model_lasso)

# erzeugen eines data frames der die Anzahl der Koeffizient mit Wert 0 in Abhängigkeit von 
# log lambda angibt
coef_zero <- cbind(log(model_lasso$lambda), dim(X_matrix)[2] - model_lasso$nzero)
plot(coef_zero, xlab = "log lambda", ylab = "Number of coefficients being zero")

# plot der Koeffizienten in Abhängigkeit von Lambda
# plot(model_lasso, xvar = "lambda")

# generiere Vorhersagen auf dem Validierungsdatensatz
# pred_cv <- predict(model_lasso, newx = as.matrix(X_matrix), s = c(0.01, 0.1), type = "response")

```

Sowohl bei der Ridge-Regression, als auch bei Lasso-Regression wird der Verlustfunktion ein Bestrafungsterm hinzugefügt der die Modellkomplexität bestraft und so die Modellkomplexität kontrollieren soll. Beide Bestrafungsterme ergeben sich in Abhängigkeit der Höhe der im Modell verwendeten Koeffizienten. Bei der Lasso-Regression wird die Summe des Betrags der Koeffizienten (also die L-1 Norm), bei der Ridge-Regression wird die Summe der Quadrats der Koeffizienten (also die L-2 Norm) als Bestrafungsterm verwendet. Das Ausmaß des Bestrafungsterms wird über einen Komplexitätsparameter lambda bestimmt. Bei Lasso schrumpfen die Koeffizienten bei größerem lambda linear gegen 0, so dass die Merkmale komplett aus dem Modell verschwinden. Es findet also eine Variablenselektion statt. Bei Ridge werden die Koeffizienten mit wachsendem lambda zwar kleiner, werden aber selten 0.

Die mittlere quadratische Abweichung verläuft mit steigendem log-lambda zuerst relativ konstant bevor sie dann steil ansteigt. Die Anzahl der Koeffizienten die Null sind nimmt dagegen mit steigendem log-lambda auch bereits zu Beginn zu. Es können, bei der passenden Wahl von lambda, zum Beispiel bei dem größten lambda das innerhalb einer Standardabweichung des besten Modells liegt, die Anzahl der Modellparameter und damit die Modellkomplexität deutlich reduziert werden, ohne dass die Vorhersagegenauigkeit des Modells (wesentlich) verschlechtert wird.

### 3.2.3 Das Elastic-Net-Verfahren

### A-27: Als abschließendes lineares Modell soll das Elastic-Net-Verfahren sachgerecht angewendet werden. Wie hängt dieses mit der Ridge-Regression und LASSO-Regression zusammen? Auch hier sollen die Hyperparameter über eine mehrdimensionale "Randomized Search" bestimmt werden. Welche Vor- und Nachteile hat dieses Vorgehen gegenüber anderen Optimierungsverfahren und welche Implikationen ergeben sich für die praktische Anwendbarkeit? (Lernziel 4.1.2) - __[10&nbsp;Punkte]__

Lösungsvorschlag A-27:

```{r A-27}
# TODO : randomized search über
# alpha
# lambda 1 ?
# lambda 2 ?
set.seed(seed)
model_glmnet <- cv.glmnet(
  x = as.matrix(X_matrix),
  y = y,
  family = "gaussian",
  alpha = 0.5,
  standardize = TRUE,
  nfolds = 10
)


```

Das Elastic-Net ist eine Kombination aus Ridge-Regression und Lasso-Regression. Beide Bestrafungsterme werden hinzugefügt und mit einem zusätzliche Parameter `alpha` gewichtet. Über die passende Kalibrerung von `alpha` können so also die Vorteile der beiden Verfahren kombiniert werden.


## 3.3. Vorwärtsgerichtetes Neuronales Netz mit Keras/TensorFlow

### A-28: Als letztes Modell wird ein vorwärtsgerichtetes künstliches neuronales Netz (MLP) herangezogen, das mit Keras/Tensorflow mit zwei bis drei verborgenen Schichten mit mindestens 35.000 zu trainierenden Gewichten sowie dropout-Regularisierung zu implementieren ist. Die gewählte Startarchitektur, die Optimierungsstrategie sowie der Umgang mit „Over-Fitting“ und schwankenden Prognoseergebnissen sollen erläutert werden. 

Lösungsvorschlag A-28:

### Die als optimal erachtete finale Netzarchitektur soll insgesamt drei Mal vollständig neu (d.h. Definition, Initialisierung & Zufallszahlen, Kompilierung, Ausführung, Evaluierung) im Notebook nacheinander angewandt und die Validierungsergebnisse angezeigt werden. Die erzielte Modellgüten und deren Varianz sollen insbesondere mit den Ergebnissen der Gradient-Boosting-Verfahren verglichen und unter Einbeziehung des Modellierungsaufwands eine vergleichende Bewertung sowie eine abschließende Modellempfehlung dokumentiert werden. (Lernziele 3.2.2, 4.1.7, 4.3.1) - __[20&nbsp;Punkte]__

Lösungsvorschlag A-29:

```{r A-29}

```

# 4. Blending und Scoring

### A-29: Abschließend ist die Prognosegüte (MAE) der Modelle graphisch zu vergleichen. Welche Schlussfolgerung ergibt sich für ein finales Ensemble der Lerner? (Lernziele 4.1.5, 4.1.7) - __[5&nbsp;Punkte]__

```{r}

```

### A-30: Es ist ein finales Ensemble aus mindestens zwei Modellklassen zu konstruieren. Welche Modellgüte ergibt sich an den Validierungsdaten und wie ist diese im Vergleich zu den einzelnen Modellen einzuordnen?  Welche praktischen Implikationen im Hinblick auf die Nutzbarkeit ergeben sich im Hinblick auf das Life-Cycle-Management von zusammengesetzten Lernern und wie sind diese mit praktischen Anforderungen der Wiederverwendbarkeit (also bspw. Re-Training) und Wartbarkeit des Codes zu vereinbaren? (Lernziel 4.1.7) - __[8&nbsp;Punkte]__

```{r}

```

### A-31: Das finale Ensemble aus Aufgabe 30 soll auf die Testdaten angewendet werden. Welche Modellgüte ergibt sich? Worauf können Unterschiede zur in Aufgabe 30 ermittelten Modellgüte zurückzuführen sein? Des weiteren soll überprüft werden, ob die an den Testdaten durchgeführte Schadenaufwandsprognose insgesamt erwartungstreu ist und erklärt werden, worauf der beobachtete Unterschied zurückzufügen ist und welche Konsequenzen das für eine Anwendung in der Tarifierung hätte.  (Lernziel 4.1.7) - __[8&nbsp;Punkte]__

```{r}

```

